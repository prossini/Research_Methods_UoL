---
title: "Week 9 Collecting Twitter Data With R"
author: "Patricia Rossini, University of Liverpool"
date: "November 22, 2021"
output: 
  html_document:
    toc: true

knit: (function(input_file, encoding) {
   rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), 'week9.html'))})


---
```{r setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(include = FALSE)
require("knitr")
library(rtweet)
opts_knit$set(root.dir = "~/Documents/GitHub/Research_Methods_UoL/")
chooseCRANmirror(graphics=FALSE, ind=1)


load("~/R/tweet-collections-credentialsR.RData")
load("~/Documents/GitHub/Research_Methods_UoL/leaders_tweets.RData") # here we are loading the data you saved
load("~/Documents/GitHub/Research_Methods_UoL/COP26_tweets_16112021.RData") # here we are loading the data you saved

```


## Introduction 

Welcome to week 9 of our Research Methods in Media and Politics module. 

This week, we are going to use R to collect data from Twitter. 
You should have already created a developer account with essential access and set up one application.

From your [developer's dashboard on Twitter](https://developer.twitter.com/en/portal/dashboard), go to your new application, click on the 'gear' symbol to go to app settings, then click on keys and tokens and generate (or regenerate) the consumer keys. 
Copy the app secret and app key to a word file. You will always need them to connect with the Twitter API.



### Installing packages
First things first: open RStudio and install all the packages we will need for this course running the following code:

```{r installing packages, message=FALSE, eval=FALSE }
install.packages("descr", "rtweet", "tidyverse", "tidytext", "glue", "stringr", "wordcloud", "lubridate")
```


As you already know, it is good practice to call your packages in the beginning of your script by using the library command. You may also want to set your work directory and a few options, such as removing scientific notations.


```{r calling packages, echo=FALSE, message=FALSE}
# options
options(scipen=999, digits = 4)
# call required packages
library(rtweet)
library(tidyverse)
library(lubridate)
library(glue)
library(descr)
library(stringr)
library(tidytext)
```




## Collecting Twitter Data:

Twitter is among the easiest/most open social media platforms to collect data from. You can get data from Twitter using the API (application programming interface), which require creating a [developers account](http://dev.twitter.com). 

After your account is ready, create a project and an app. 

There are several R packages that interact with Twitter's API. We will use rtweet, by [Michael Kearney](https://rtweet.info/articles/intro.html), which can query both the REST and the stream APIs.

Now, you need to authenticate with Twitter using the credentials you created for your app:

```{r twitter oauth, eval=FALSE, include=TRUE}


## paste use your credentials between quotes. 
## Create a token

token <- create_token(
  app = app_name, #your app name
  consumer_key = app_key,#consumer key for your app, replace app_key with your app's key between quotes
  consumer_secret = app_secret)#consumer secret for your app, app_secret with your app's secret between quotes
 
```

There are different types of Twitter data you can collect. 

For the purposes of this workshop and the hands-on work we do here, we will focus a bit more on the REST API, which collects tweets from a timeline (up to 3200) and from a search (of hashtags, keywords etc), as well as networks of users -- e.g. followers / followed accounts.

Your free developer account gives you essential access to Twitter data, with a limit of 500,000 tweets per month. If you decide to use Twitter data for your dissertation, you can also apply for [https://developer.twitter.com/en/products/twitter-api/academic-research](academic access), which can give you access to 10 million tweets per month for free. 


### Timeline Search


Let's begin with timeline searches, that is, retrieving up to 3200 tweets from a single (or multiple) public accounts. 
You would use these commands to collect the content of tweets by an account, including data about engagement (likes, retweets).
For instance, let's compare tweets by Boris Johnson & the Conservative party vs tweets by Keir Starmer & Labour


```{r search API, eval=FALSE, include=TRUE}

## getting tweets from one user and assigning them to an object named 'Boris'
## 
boris <- get_timeline("BorisJohnson", n=3200, retryOnRateLimit=120, resultType = "recent")


## getting tweets from several users and assigning them to a single object named 'leaders'
leaders <- get_timelines(c("BorisJohnson", "Keir_Starmer", "Conservatives", "UKLabour") , n=3200, retryOnRateLimit=120, resultType = "recent")

# if we want to save this data to work on it later, just use the following command: 
save(leaders, file = "leaders_tweets_16112021.RData") # to save it as an R object OR
write_as_csv(leaders, file_name = "leaders_tweets_16112021.csv") # to save as a CSV that can be opened in excel. This function is from the package rtweet, not the same as 'write.csv' from base R. 

``` 

Note that I added the date I collected the data to the filename before saving. This is just for 'housekeeping': it is easier to remember when you created a data collection if you keep track of dates in filenames.


Now we have a dataframe with 90 variables and over 12,000 Tweets from the four accounts.  
As you learned before, the first step is to inspect our data by looking at the column names:

```{r checking the df, eval=FALSE}
colnames(leaders)
```


With this command, we can see the different columns in our dataframe. We have engagement data (e.g. likes, retweets), data about devices used to tweet, geolocation etc. You can check Twitter's [documentation](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet) to see what each of these columns mean if you are curious. Unlike our survey datasets, however, it is a bit easier to guess what each column contains by looking at the column name.


We can also view our dataframe in RStudio by clickong on it in the Environment tab, or inspect the first few columns using: 


```{r inspect, echo=TRUE, eval=FALSE}

head(leaders)

```


### Inspecting our data


We downloaded tweets from four different accounts, so it is useful to inspect the data. 
Let's look at how many tweets we have per account. It also makes sense to check the earliest date of each account's tweets so that we can filter our data afterwards to make sure we are comparing accounts in the same period of time. 



```{r}

freq(leaders$screen_name) # unsurprisingly, we have about 3200 for all accounts, as this is the maximum we can retrieve using the free API

#let's check the dates. 
#First we convert the date column to a 'date' object using the function as.Date:
leaders$created_at <- as.Date(leaders$created_at)

#then, we check the earliest and latest tweet date per account:
leaders %>% group_by(screen_name) %>% summarise(min(created_at), max(created_at))

# from this we notice that tweets by Boris Johnson go way back to July, 2019, but all others were some time in 2020. 
# To make our dataset comparable, let's limit the analysis to tweets posted in 2021, filtering by date. 
# I will assign the filtered dataset to a new object just in case we want to return to the full set some other time. 

leaders_analysis <- leaders %>% filter(created_at >= '2021-01-01') #now our new data has 7040 tweets. Let's check again the dates to make sure it looks correct: 
leaders_analysis %>% group_by(screen_name) %>% summarise(min(created_at), max(created_at)) # great, now all our accounts have tweets for the same period of time! 

```


This first part was about inspecting the data and understanding what we collected, as well as filtering data by the created date to narrow down our tweets to 2021. 


### Hashtags and searching terms


Now, a different type of data we can get is a sample of tweets about a topic or hashtag, using search terms and search operators. You can learn more about which different [search operators](https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators) can be used on Twitter. 

In this example, let's make a simple search to collect tweets using the hashtag #COP26.

Bear in mind this is a sample of tweets (capped at 1% of all tweets at the moment of the query) and we don't have a way to know the 'population'--that is, how many people were actually using this hashtag at any given time. As such, any analysis or results would have to account for the limitation of using a sample of an unkrnown population. 


```{r search tweets, include=TRUE, eval=FALSE, inspect=FALSE}
COP26 <- search_tweets(
  "#cop26",include_rts = FALSE, retryonratelimit = TRUE) # here we are asking the API for tweets using this hashtag, excluding RTs


save(COP26, file = "COP26_tweets_16112021.RData") # to save it as an R object OR
write_as_csv(COP26, file_name = "leaders_tweets_16112021.csv") # to save as a CSV that can be opened in excel. This function is from the package rtweet, 
```


If we inspect this dataset (using colnames, or clicking on the environment), you will see that here we have tweets by many different accounts that used the hashtag #COP26. The difference from the first dataset we collected is that instead of getting 3200 tweets by a defined number of accounts, in this search we are getting a sample of recent tweets using this hashtag that have been posted by any account.  



## Data Analysis


There are different analytic techniques you can use to study twitter data depending on your research interests. To study the content of tweets, you could use manual content analysis (as you learned in Week 8) for instance to study how a newspaper or a politician talk about something on twitter or even more generally assess their overall communication strategies. In this workshop, we are only covering a few possibilities to give you an idea of research projects using Twitter data. 



### Engagement: comparing mean engagement per account

Let's bring together what you learned in weeks 5 & 6 with your newly acquired data collection skills.
The twitter dataset contains several numeric variables that count engagement metrics, such as retweets and likes. 
So we could look into basic descriptive statistics to further compare and analyse these accounts: 


```{r means}
leaders_analysis %>% group_by(screen_name) %>% 
  summarise(mean(retweet_count), mean(favorite_count))
```

From these, we can clearly see that the party leaders receive far more retweets and likes than their own parties in 2021. While tweets by Johnson received, on average, 614 retweets in 2021, his own party got a fraction of that, with around 261 RTs per tweet. A similar pattern can be observed with Starmer versus Labour. 


### Time Series

Time series are useful to understand data patterns over time. This can help you compare how much different accounts tweet over time, or investigate tweets (or engagements) over a period of time. The main point of a time series plot is to show trends over a certain period of time.


For instance, considering the leaders dataset we collected, we can plot the four accounts' Twitter activity in 2021 to answer basic research questions such as 1) were parties or leaders more active on twitter in 2021? or 2)  

```{r, include=TRUE}

leaders_analysis %>%
  group_by(screen_name) %>% 
  rtweet::ts_plot("weeks", trim = 2L) +
  geom_abline() +
  theme_classic() +
  scale_x_datetime(date_labels = "%b %d", breaks = "2 week") +
  scale_color_brewer(type = "qual", palette = 2) +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position = "bottom",
    plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "@BorisJohnson, @Conservatives, @Keir_Starmer and @Labour on Twitter, 2021",
    subtitle = "Aggregated by 2 weeks")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

You can tweak your plot changing the colors, breaks (hours, days, weeks...), the type of the plot, the theme etc. Check [ggplot2](https://ggplot2.tidyverse.org/reference/) reference to learn more about these options. 


You can also filter the data further to facilitate interpretation. For instance, comparing just the party leaders:
```{r, include=TRUE}

leaders_analysis %>%
  group_by(screen_name) %>% 
  filter(screen_name == "BorisJohnson" | screen_name == "Keir_Starmer") %>% 
  rtweet::ts_plot("weeks", trim = 2L) +
  geom_abline() +
  theme_classic() +
  scale_x_datetime(date_labels = "%b %d", breaks = "2 week") +
  scale_color_brewer(type = "qual", palette = 2) +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position = "bottom",
    plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "@BorisJohnson x  @Keir_Starmer on Twitter, 2021",
    subtitle = "Aggregated by 2 weeks")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Here, it becomes easier to compare the different patterns of tweets between the two leaders. 

What does this plot tell you? Which conclusions would you draw?


### Hashtags and Frequent Words

Another interesting analysis with Twitter is looking at hashtags, since hashtags group tweets under a searchable topic and can signal the topic of a tweet. 

For our UK politicians data, 


```{r echo=TRUE, include=TRUE}
# first, because a tweet can have many hashtags, these are collected as a list. 
# You can see that by looking at the dataset: when you see more than one hashtag in the hashtag column, it is represented by c("hastag1", "hashtag2"), meaning it is a list. The code below will identify hashtags using the function unnest_tokens from the package tidytext
# with the code below, we are grouping the data to see the top 10 hashtags per account and assigning the results to a new dataframe.
leader_hashtag <- leaders_analysis %>% 
  group_by(screen_name) %>%
  unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(hashtag, "^#")) %>%
  count(hashtag, sort = TRUE) %>%
  top_n(10)

## then we can plot these results to visualize:
leader_hashtag %>%
  mutate(hashtag = reorder(hashtag, n)) %>%
  ggplot(aes(x = hashtag, y = n, fill = screen_name)) +
  geom_col(position = "dodge") +
  coord_flip() + 
  labs(x = "Count",
       y = "Hashtag",
       title = "Top 10 Popular Hashtags per Political Account")

```


We can also do the same for our #COP26 dataset. Which other hashtags are used in these tweets? 
```{r echo=TRUE, include=TRUE}
# with the code below, we are grouping the data to see the top 10 hashtags per account and assigning the results to a new dataframe.
COP_hashtags <- COP26 %>% 
  unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(hashtag, "^#"), hashtag != "#COP26", hashtag != "#cop26", hashtag != "#Cop26") %>% #in this line we are filtering the hashtag #COP26 because we already know all tweets will have it
  count(hashtag, sort = TRUE) %>%
  top_n(20)

## then we can plot these results to visualize:
COP_hashtags %>%
  mutate(hashtag = reorder(hashtag, n)) %>%
  ggplot(aes(x = hashtag, y = n)) +
  geom_col(position = "dodge") +
  coord_flip() + 
  labs(x = "Count",
       y = "Hashtag",
       title = "Top 20 Popular Hashtags Tweeted Alongside #COP26")

```


# That's all (for now)!

This workshop only focused on getting tweets based on accounts and hashtags as these are the most likely use-cases for your own dissertations, but you can find many more use cases by looking at the [documentation for RTweet](https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html)

If you want to keep learning R and use Twitter data in your dissertation, you can also explore more automated techniques that we will not to cover in the module. 

For instance, you could use textual mining and analysis techniques if you want to find which words are more frequently associated with a particular hashtag. You can also consider network analysis techniques if you want to study relationships between accounts. There are several online tutorials that will teach you how to implement these techniques on tweets using R.  

